{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f70d567-8cda-49db-9cf8-25107940799f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Train Loss: 0.0299 | Val Loss: 0.0073\n",
      "Epoch [2/200] | Train Loss: 0.0060 | Val Loss: 0.0051\n",
      "Epoch [3/200] | Train Loss: 0.0043 | Val Loss: 0.0034\n",
      "Epoch [4/200] | Train Loss: 0.0035 | Val Loss: 0.0029\n",
      "Epoch [5/200] | Train Loss: 0.0031 | Val Loss: 0.0025\n",
      "Epoch [6/200] | Train Loss: 0.0028 | Val Loss: 0.0023\n",
      "Epoch [7/200] | Train Loss: 0.0027 | Val Loss: 0.0023\n",
      "Epoch [8/200] | Train Loss: 0.0026 | Val Loss: 0.0028\n",
      "Epoch [9/200] | Train Loss: 0.0025 | Val Loss: 0.0038\n",
      "Epoch [10/200] | Train Loss: 0.0024 | Val Loss: 0.0030\n",
      "Epoch [11/200] | Train Loss: 0.0024 | Val Loss: 0.0026\n",
      "Epoch [12/200] | Train Loss: 0.0023 | Val Loss: 0.0025\n",
      "Epoch [13/200] | Train Loss: 0.0023 | Val Loss: 0.0019\n",
      "Epoch [14/200] | Train Loss: 0.0022 | Val Loss: 0.0024\n",
      "Epoch [15/200] | Train Loss: 0.0022 | Val Loss: 0.0028\n",
      "Epoch [16/200] | Train Loss: 0.0022 | Val Loss: 0.0019\n",
      "Epoch [17/200] | Train Loss: 0.0021 | Val Loss: 0.0019\n",
      "Epoch [18/200] | Train Loss: 0.0021 | Val Loss: 0.0019\n",
      "Epoch [19/200] | Train Loss: 0.0021 | Val Loss: 0.0017\n",
      "Epoch [20/200] | Train Loss: 0.0021 | Val Loss: 0.0022\n",
      "Epoch [21/200] | Train Loss: 0.0020 | Val Loss: 0.0017\n",
      "Epoch [22/200] | Train Loss: 0.0020 | Val Loss: 0.0020\n",
      "Epoch [23/200] | Train Loss: 0.0020 | Val Loss: 0.0018\n",
      "Epoch [24/200] | Train Loss: 0.0020 | Val Loss: 0.0017\n",
      "Epoch [25/200] | Train Loss: 0.0020 | Val Loss: 0.0023\n",
      "Epoch [26/200] | Train Loss: 0.0020 | Val Loss: 0.0019\n",
      "Epoch [27/200] | Train Loss: 0.0020 | Val Loss: 0.0016\n",
      "Epoch [28/200] | Train Loss: 0.0019 | Val Loss: 0.0019\n",
      "Epoch [29/200] | Train Loss: 0.0019 | Val Loss: 0.0028\n",
      "Epoch [30/200] | Train Loss: 0.0019 | Val Loss: 0.0022\n",
      "Epoch [31/200] | Train Loss: 0.0019 | Val Loss: 0.0015\n",
      "Epoch [32/200] | Train Loss: 0.0019 | Val Loss: 0.0017\n",
      "Epoch [33/200] | Train Loss: 0.0019 | Val Loss: 0.0023\n",
      "Epoch [34/200] | Train Loss: 0.0019 | Val Loss: 0.0018\n",
      "Epoch [35/200] | Train Loss: 0.0018 | Val Loss: 0.0020\n",
      "Epoch [36/200] | Train Loss: 0.0018 | Val Loss: 0.0016\n",
      "Epoch [37/200] | Train Loss: 0.0018 | Val Loss: 0.0018\n",
      "Epoch [38/200] | Train Loss: 0.0018 | Val Loss: 0.0019\n",
      "Epoch [39/200] | Train Loss: 0.0018 | Val Loss: 0.0017\n",
      "Epoch [40/200] | Train Loss: 0.0018 | Val Loss: 0.0020\n",
      "Epoch [41/200] | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch [42/200] | Train Loss: 0.0017 | Val Loss: 0.0016\n",
      "Epoch [43/200] | Train Loss: 0.0017 | Val Loss: 0.0018\n",
      "Epoch [44/200] | Train Loss: 0.0017 | Val Loss: 0.0020\n",
      "Epoch [45/200] | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch [46/200] | Train Loss: 0.0017 | Val Loss: 0.0014\n",
      "Epoch [47/200] | Train Loss: 0.0016 | Val Loss: 0.0014\n",
      "Epoch [48/200] | Train Loss: 0.0016 | Val Loss: 0.0016\n",
      "Epoch [49/200] | Train Loss: 0.0016 | Val Loss: 0.0027\n",
      "Epoch [50/200] | Train Loss: 0.0016 | Val Loss: 0.0015\n",
      "Epoch [51/200] | Train Loss: 0.0016 | Val Loss: 0.0014\n",
      "Epoch [52/200] | Train Loss: 0.0016 | Val Loss: 0.0019\n",
      "Epoch [53/200] | Train Loss: 0.0016 | Val Loss: 0.0017\n",
      "Epoch [54/200] | Train Loss: 0.0016 | Val Loss: 0.0013\n",
      "Epoch [55/200] | Train Loss: 0.0016 | Val Loss: 0.0014\n",
      "Epoch [56/200] | Train Loss: 0.0016 | Val Loss: 0.0012\n",
      "Epoch [57/200] | Train Loss: 0.0016 | Val Loss: 0.0014\n",
      "Epoch [58/200] | Train Loss: 0.0016 | Val Loss: 0.0021\n",
      "Epoch [59/200] | Train Loss: 0.0016 | Val Loss: 0.0014\n",
      "Epoch [60/200] | Train Loss: 0.0016 | Val Loss: 0.0014\n",
      "Epoch [61/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [62/200] | Train Loss: 0.0016 | Val Loss: 0.0017\n",
      "Epoch [63/200] | Train Loss: 0.0015 | Val Loss: 0.0016\n",
      "Epoch [64/200] | Train Loss: 0.0015 | Val Loss: 0.0015\n",
      "Epoch [65/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [66/200] | Train Loss: 0.0015 | Val Loss: 0.0012\n",
      "Epoch [67/200] | Train Loss: 0.0015 | Val Loss: 0.0020\n",
      "Epoch [68/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [69/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [70/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [71/200] | Train Loss: 0.0015 | Val Loss: 0.0012\n",
      "Epoch [72/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [73/200] | Train Loss: 0.0015 | Val Loss: 0.0012\n",
      "Epoch [74/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [75/200] | Train Loss: 0.0015 | Val Loss: 0.0018\n",
      "Epoch [76/200] | Train Loss: 0.0015 | Val Loss: 0.0012\n",
      "Epoch [77/200] | Train Loss: 0.0015 | Val Loss: 0.0018\n",
      "Epoch [78/200] | Train Loss: 0.0015 | Val Loss: 0.0016\n",
      "Epoch [79/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [80/200] | Train Loss: 0.0015 | Val Loss: 0.0013\n",
      "Epoch [81/200] | Train Loss: 0.0015 | Val Loss: 0.0017\n",
      "Epoch [82/200] | Train Loss: 0.0015 | Val Loss: 0.0018\n",
      "Epoch [83/200] | Train Loss: 0.0015 | Val Loss: 0.0014\n",
      "Epoch [84/200] | Train Loss: 0.0015 | Val Loss: 0.0012\n",
      "Epoch [85/200] | Train Loss: 0.0015 | Val Loss: 0.0012\n",
      "Epoch [86/200] | Train Loss: 0.0015 | Val Loss: 0.0014\n",
      "Epoch [87/200] | Train Loss: 0.0014 | Val Loss: 0.0016\n",
      "Epoch [88/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [89/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [90/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [91/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [92/200] | Train Loss: 0.0014 | Val Loss: 0.0016\n",
      "Epoch [93/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [94/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [95/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [96/200] | Train Loss: 0.0014 | Val Loss: 0.0018\n",
      "Epoch [97/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [98/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [99/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [100/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [101/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [102/200] | Train Loss: 0.0014 | Val Loss: 0.0017\n",
      "Epoch [103/200] | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "Epoch [104/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [105/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [106/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [107/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [108/200] | Train Loss: 0.0014 | Val Loss: 0.0019\n",
      "Epoch [109/200] | Train Loss: 0.0014 | Val Loss: 0.0016\n",
      "Epoch [110/200] | Train Loss: 0.0014 | Val Loss: 0.0016\n",
      "Epoch [111/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [112/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [113/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [114/200] | Train Loss: 0.0014 | Val Loss: 0.0015\n",
      "Epoch [115/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [116/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [117/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [118/200] | Train Loss: 0.0014 | Val Loss: 0.0025\n",
      "Epoch [119/200] | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "Epoch [120/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [121/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [122/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [123/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [124/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [125/200] | Train Loss: 0.0014 | Val Loss: 0.0016\n",
      "Epoch [126/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [127/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [128/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [129/200] | Train Loss: 0.0014 | Val Loss: 0.0015\n",
      "Epoch [130/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [131/200] | Train Loss: 0.0014 | Val Loss: 0.0012\n",
      "Epoch [132/200] | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "Epoch [133/200] | Train Loss: 0.0014 | Val Loss: 0.0013\n",
      "Epoch [134/200] | Train Loss: 0.0014 | Val Loss: 0.0015\n",
      "Epoch [135/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [136/200] | Train Loss: 0.0014 | Val Loss: 0.0011\n",
      "Epoch [137/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [138/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [139/200] | Train Loss: 0.0013 | Val Loss: 0.0017\n",
      "Epoch [140/200] | Train Loss: 0.0013 | Val Loss: 0.0010\n",
      "Epoch [141/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [142/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [143/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [144/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [145/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [146/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [147/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [148/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [149/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [150/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [151/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [152/200] | Train Loss: 0.0013 | Val Loss: 0.0014\n",
      "Epoch [153/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [154/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [155/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [156/200] | Train Loss: 0.0013 | Val Loss: 0.0021\n",
      "Epoch [157/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [158/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [159/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [160/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [161/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [162/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [163/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [164/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [165/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [166/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [167/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [168/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [169/200] | Train Loss: 0.0013 | Val Loss: 0.0014\n",
      "Epoch [170/200] | Train Loss: 0.0013 | Val Loss: 0.0010\n",
      "Epoch [171/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [172/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [173/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [174/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [175/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [176/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [177/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [178/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [179/200] | Train Loss: 0.0013 | Val Loss: 0.0010\n",
      "Epoch [180/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [181/200] | Train Loss: 0.0013 | Val Loss: 0.0010\n",
      "Epoch [182/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [183/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [184/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [185/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [186/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [187/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [188/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [189/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [190/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [191/200] | Train Loss: 0.0013 | Val Loss: 0.0010\n",
      "Epoch [192/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [193/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [194/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [195/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [196/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n",
      "Epoch [197/200] | Train Loss: 0.0013 | Val Loss: 0.0012\n",
      "Epoch [198/200] | Train Loss: 0.0013 | Val Loss: 0.0018\n",
      "Epoch [199/200] | Train Loss: 0.0013 | Val Loss: 0.0013\n",
      "Epoch [200/200] | Train Loss: 0.0013 | Val Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 自定义数据集类\n",
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_csv(data_path).values\n",
    "        self.X = data[:, :84].astype(np.float32)   # 输入：前84位\n",
    "        self.y = data[:, -42:].astype(np.float32)   # 输出：后42位真实数据\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 基于1D卷积的自编码器\n",
    "class Conv1DAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 编码器：1D卷积提取时间特征\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # 输入形状: (batch, 1, 84)\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出形状: (batch, 16, 42)\n",
    "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # 输出形状: (batch, 32, 21)\n",
    "        )\n",
    "        # 解码器：反卷积还原时间特征\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=2, stride=2),  # 输出形状: (batch, 16, 42)\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=32, out_channels=1, kernel_size=2, stride=2),  # 输出形状: (batch, 1, 84)\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 42)  # 将84维映射到42维\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # 增加通道维度：(batch, 1, 84)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        decoded = decoded.squeeze(1)  # 移除通道维度：(batch, 42)\n",
    "        return decoded\n",
    "\n",
    "# 训练参数设置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "\n",
    "# 加载数据集并划分训练集和验证集\n",
    "dataset = SensorDataset(\"log.csv\")  # 替换为你的数据路径\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = Conv1DAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练循环\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_scripted = torch.jit.script(model)\n",
    "        \n",
    "        en_in_dim = model.encoder[0].in_channels\n",
    "        mid_dim = model.decoder[0].in_channels\n",
    "        de_ou_dim = model.decoder[-1].out_features\n",
    "        \n",
    "        \n",
    "        model_scripted.save(f'best_conv1d_model_{en_in_dim}_{de_ou_dim}.pt')\n",
    "        torch.save(model.encoder.state_dict(), f'best_conv1d_encoder_{en_in_dim}_{mid_dim}.pt')\n",
    "        torch.save(model.decoder.state_dict(), f'best_conv1d_decoder_{mid_dim}_{de_ou_dim}.pt')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# 加载最佳模型进行测试或使用\n",
    "# model.load_state_dict(torch.load('best_conv1d_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08515f20-2dfe-4e08-bad5-2aed05e29ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_scripted = torch.jit.script(model.encoder)\n",
    "encoder_scripted.save('best_conv1d_encoder_1_16.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "012ec79b-2374-49fe-bb78-0d0b8251410e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The provided filename best_conv1d_model.pt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_conv1d_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 设置为评估模式（禁用 dropout/batch norm 等）\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/jit/_serialization.py:149\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(f):  \u001b[38;5;66;03m# type: ignore[type-var]\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided filename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[str-bytes-safe]\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(f):\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided filename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[str-bytes-safe]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The provided filename best_conv1d_model.pt does not exist"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = torch.jit.load('best_conv1d_model.pt')\n",
    "# 设置为评估模式（禁用 dropout/batch norm 等）\n",
    "model.eval()\n",
    "\n",
    "import random\n",
    "# 随机选择一个批次的数据\n",
    "random_batch = random.choice(list(val_loader))\n",
    "x_test, _ = random_batch  # 取出输入数据\n",
    "x_test = x_test.to(device)\n",
    "\n",
    "# 只取部分特征进行重构\n",
    "_x_test = x_test[:,  np.r_[:84]]\n",
    "real_x_test = x_test[:, -42:]#x_test[:, -42:]  # 真实值\n",
    "\n",
    "# 进行推理\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_x_test = model(_x_test)\n",
    "\n",
    "# 转换为 NumPy 方便可视化\n",
    "_x_test_np = _x_test.cpu().numpy()\n",
    "real_x_test_np = real_x_test.cpu().numpy()\n",
    "recon_x_test_np = recon_x_test.cpu().numpy()\n",
    "\n",
    "# 假设 batch_size 和 feature_dim 已知\n",
    "batch_size = real_x_test_np.shape[0]  # 批次大小\n",
    "num_features = real_x_test_np.shape[1]  # 特征数量\n",
    "num_features = 42\n",
    "\n",
    "# 创建多个子图，每个子图表示一个元素的 real 和 recon 值\n",
    "fig, axes = plt.subplots(nrows=num_features, figsize=(12, num_features * 3), sharex=True)\n",
    "\n",
    "# 确保 axes 是一个可迭代的数组\n",
    "if num_features == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# 遍历每个特征\n",
    "for i in range(num_features):\n",
    "    axes[i].plot(np.arange(50), real_x_test_np[:50, i], label=\"ground truth\", marker='o')\n",
    "    axes[i].plot(np.arange(50), recon_x_test_np[:50, i], label=\"AE recon\", marker='x')\n",
    "    axes[i].set_title(f\"Feature {i} Real vs. Recon\")\n",
    "    axes[i].set_xlabel(\"Batch Index\")\n",
    "    axes[i].set_ylabel(\"Value\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc17dd82-d2de-47c6-b287-f599ae3c0d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_graph.png'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 创建一个随机输入\n",
    "dummy_input = torch.randn(1, 84, device=device)  # 输入形状: (batch_size, 84)\n",
    "\n",
    "# 获取模型输出\n",
    "output = model(dummy_input)\n",
    "\n",
    "# 生成计算图\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# 保存为文件或直接显示\n",
    "dot.format = 'png'  # 保存为PNG格式\n",
    "dot.render('model_graph')  # 保存文件并渲染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21d3689-ed9a-46c0-acde-da9c04473bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 64, 84]             256\n",
      "       BatchNorm1d-2               [-1, 64, 84]             128\n",
      "              ReLU-3               [-1, 64, 84]               0\n",
      "         MaxPool1d-4               [-1, 64, 42]               0\n",
      "            Conv1d-5               [-1, 32, 42]           6,176\n",
      "       BatchNorm1d-6               [-1, 32, 42]              64\n",
      "              ReLU-7               [-1, 32, 42]               0\n",
      "         MaxPool1d-8               [-1, 32, 21]               0\n",
      "   ConvTranspose1d-9               [-1, 64, 42]           4,160\n",
      "      BatchNorm1d-10               [-1, 64, 42]             128\n",
      "             ReLU-11               [-1, 64, 42]               0\n",
      "  ConvTranspose1d-12                [-1, 1, 84]             129\n",
      "      BatchNorm1d-13                [-1, 1, 84]               2\n",
      "             ReLU-14                [-1, 1, 84]               0\n",
      "           Linear-15                [-1, 1, 42]           3,570\n",
      "================================================================\n",
      "Total params: 14,613\n",
      "Trainable params: 14,613\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# 显示模型结构\n",
    "summary(model, input_size=(84,))  # 输入形状: (84,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc08d0-80da-4bdd-964a-1e2b9e9c69b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
